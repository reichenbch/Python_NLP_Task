{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35536\n"
     ]
    }
   ],
   "source": [
    "with open(\"MSDialog-Complete.json\",\"r\") as jf:\n",
    "    msd_data = json.load(jf)\n",
    "    print(len(msd_data))\n",
    "    msd_text = []\n",
    "    for data in msd_data:\n",
    "        for utt_dict in msd_data[data]['utterances']:\n",
    "            msd_text.append(utt_dict['utterance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"\\.\", \"\", text)\n",
    "    text = re.sub(r\"!\", \"\", text)\n",
    "    text = re.sub(r\"\\/\", \"\", text)\n",
    "    text = re.sub(r\"\\^\", \"\", text)\n",
    "    text = re.sub(r\"\\+\", \"\", text)\n",
    "    text = re.sub(r\"\\-\", \"\", text)\n",
    "    text = re.sub(r\"\\=\", \"\", text)\n",
    "    text = re.sub(r\";\",\"\",text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \"\", text)\n",
    "    #text = re.sub(r\" e g \", \" eg \", text)\n",
    "    #text = re.sub(r\" b g \", \" bg \", text)\n",
    "    #text = re.sub(r\" u s\", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    #text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    #text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tData in range(len(msd_text)):\n",
    "    temp = clean_text(msd_text[tData])\n",
    "    msd_text[tData] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msd_text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def binaryEq(n):\n",
    "    a=[]\n",
    "    while(n>0):\n",
    "        dig=n%2;a.append(dig);n=n//2\n",
    "    a.reverse()\n",
    "    return a\n",
    "def generateIndexVector(text):\n",
    "    temp = 0\n",
    "    temp_embed = [0] * 1024\n",
    "    for k in range(len(text)):\n",
    "        temp += ord(text[k])*k\n",
    "    mapCheck = binaryEq(temp)\n",
    "    checkSum = int(0.05*1024);indexCheck = 0\n",
    "    random.seed()\n",
    "    for i in range(len(mapCheck)):\n",
    "        if((mapCheck[i]==1) and (indexCheck<checkSum)):\n",
    "            index = random.randint(0,1023)\n",
    "            temp_embed[index] = 1\n",
    "            indexCheck += 1\n",
    "    return temp_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conflict Detected pciid 03035\n",
      "Conflict Detected rihanna 1650100\n",
      "Conflict Detected nadam 41216\n",
      "Conflict Detected wifid 4ea5869a49a64e8a91f5bb904696262f\n",
      "Conflict Detected weekd vehid\n",
      "Conflict Detected 61216 280x\n",
      "Conflict Detected 50a2f9e3 52262\n",
      "Conflict Detected 20h majid\n",
      "Conflict Detected 20h 32822\n",
      "Conflict Detected cubic rubic\n",
      "Conflict Detected 1772316 rsrc\n",
      "Conflict Detected havea 71813\n",
      "Conflict Detected 5460000 sv1ts\n",
      "Conflict Detected 0x000000000001301a lni\n",
      "Conflict Detected tyra 7537\n",
      "Conflict Detected 0x16d4000 11017\n",
      "Conflict Detected 31216 14f\n",
      "Conflict Detected telah rehid\n",
      "Conflict Detected whena 71017\n",
      "Conflict Detected 6017335 safesearchcom\n",
      "Conflict Detected 22225 p2g\n",
      "Conflict Detected 85658 zejm\n",
      "Conflict Detected 83116 pedal\n",
      "Conflict Detected 16821 f2g\n",
      "Conflict Detected 82144 staff\n",
      "Conflict Detected 61415 reich\n"
     ]
    }
   ],
   "source": [
    "def generateEmbeddings(msd_text):\n",
    "    embed_dict = dict()\n",
    "    vect_dict = dict()\n",
    "    word_set = set()\n",
    "    collided_words = set()\n",
    "    count = 0\n",
    "    for i in range(len(msd_text)):\n",
    "        utt_data = msd_text[i]\n",
    "        #print(utt_data)\n",
    "        utt_data = utt_data.split(' ')\n",
    "        #print(utt_data)\n",
    "        for j in range(len(utt_data)):\n",
    "            if(len(utt_data[j])>1):\n",
    "                embed  = generateIndexVector(utt_data[j])\n",
    "                checkEmbed = ''.join([str(i) for i in embed])\n",
    "                if((utt_data[j] not in embed_dict) and (checkEmbed not in vect_dict)):\n",
    "                    embed_dict[utt_data[j]] = embed\n",
    "                    vect_dict[checkEmbed] = utt_data[j]\n",
    "                    word_set.add(utt_data[j])\n",
    "                elif(utt_data[j] not in word_set):\n",
    "                    count += 1\n",
    "                    collided_words.add(utt_data[j])\n",
    "                   #print('Conflict Detected',utt_data[j],vect_dict.get(checkEmbed))\n",
    "    return embed_dict\n",
    "\n",
    "def removeCollisions(collided_words,embed_dict,vect_dict):\n",
    "    while(len(collided_words)==0):\n",
    "        new_collide = set()\n",
    "        for j in range(len(collided_words)):\n",
    "            if(len(collided_words[j])>1):\n",
    "                embed  = generateIndexVector(collided_words[j])\n",
    "                checkEmbed = ''.join([str(i) for i in embed])\n",
    "                if((collided_words[j] not in embed_dict) and (checkEmbed not in vect_dict)):\n",
    "                    embed_dict[collided_words[j]] = embed\n",
    "                    vect_dict[checkEmbed] = collided_words[j]\n",
    "                    word_set.add(collided_words[j])\n",
    "                elif(collided_words[j] not in word_set):\n",
    "                    count += 1\n",
    "                    new_collide.add(collided_words[j])\n",
    "                   #print('Conflict Detected',collided_words[j],vect_dict.get(checkEmbed))\n",
    "        collided_words = new_collide\n",
    "    vect_dict.clear()\n",
    "    collided_words.clear()\n",
    "    return embed_dict\n",
    "\n",
    "embed_dict = generateEmbeddings(msd_text)\n",
    "embed_dict = removeCollisions(collided_words,embed_dict,vect_dict)\n",
    "import pickle\n",
    "with open(\"index_vectors.pickle\",\"wb\") as file:\n",
    "    pickle.dump(embed_dict, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "strm = 'there'\n",
    "temp = 0\n",
    "for k in range(len(strm)):\n",
    "    print(strm[k],k)\n",
    "    temp += ord(strm[k])*k\n",
    "\n",
    "import random\n",
    "mapCheck = binaryEq(temp)\n",
    "checkSum = int(0.05*1024);indexCheck = 0\n",
    "for i in range(len(mapCheck)):\n",
    "    if((mapCheck[i]==1) and (indexCheck<checkSum)):\n",
    "        index = random.randint(0,1024)\n",
    "        temp_embed[index] = 1\n",
    "        indexCheck += 1\n",
    "\n",
    "print([i for i in range(len(temp_embed)) if temp_embed[i]==1])\n",
    "\n",
    "strm = 'three'\n",
    "temp = 0\n",
    "for k in range(len(strm)):\n",
    "    print(strm[k],k)\n",
    "    temp += ord(strm[k])*k\n",
    "\n",
    "import random\n",
    "mapCheck = binaryEq(temp)\n",
    "checkSum = int(0.05*1024);indexCheck = 0\n",
    "for i in range(len(mapCheck)):\n",
    "    if((mapCheck[i]==1) and (indexCheck<checkSum)):\n",
    "        index = random.randint(0,1024)\n",
    "        temp_embed[index] = 1\n",
    "        indexCheck += 1\n",
    "\n",
    "print([i for i in range(len(temp_embed)) if temp_embed[i]==1])\n",
    "\n",
    "#Checking collisions\n",
    "[i for i in range(len(embed_dict['upgrad'])) if embed_dict['upgrad'][i]==1]\n",
    "for embed,text in vect_dict.items():\n",
    "    if(embed==(''.join([str(i) for i in embed_dict['upgrad']]))):\n",
    "        print(text)\n",
    "        \n",
    "#Removing Collisions Manually\n",
    "collided_words = [\"pciid\",\"rihanna\",\"nadam\",\"wifid\",\"weekd\",\"61216\",\"50a2f9e3\",\"20h\",\"cubic\",\"1772316\",\"havea\",\"5460000\",\"tyra\"\n",
    "                  ,\"0x16d4000\",\"31216\",\"telah\",\"whena\",\"6017335\",\"22225\",\"85658\",\"83116\",\"16821\",\"82144\",\"61415\"]\n",
    "for j in range(len(collided_words)):\n",
    "    if(len(collided_words[j])>1):\n",
    "        embed  = generateIndexVector(collided_words[j])\n",
    "        checkEmbed = ''.join([str(i) for i in embed])\n",
    "        if((collided_words[j] not in embed_dict) and (checkEmbed not in vect_dict)):\n",
    "            embed_dict[collided_words[j]] = embed\n",
    "            vect_dict[checkEmbed] = collided_words[j]\n",
    "            word_set.add(collided_words[j])\n",
    "        elif(collided_words[j] not in word_set):\n",
    "            count += 1\n",
    "            print('Conflict Detected',collided_words[j],vect_dict.get(checkEmbed))\n",
    "            \n",
    "collided_words = [\"20h\",\"61415\"]\n",
    "for j in range(len(collided_words)):\n",
    "    if(len(collided_words[j])>1):\n",
    "        embed  = generateIndexVector(collided_words[j])\n",
    "        checkEmbed = ''.join([str(i) for i in embed])\n",
    "        if((collided_words[j] not in embed_dict) and (checkEmbed not in vect_dict)):\n",
    "            embed_dict[collided_words[j]] = embed\n",
    "            vect_dict[checkEmbed] = collided_words[j]\n",
    "            word_set.add(collided_words[j])\n",
    "        elif(collided_words[j] not in word_set):\n",
    "            count += 1\n",
    "            print('Conflict Detected',collided_words[j],vect_dict.get(checkEmbed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256777\n"
     ]
    }
   ],
   "source": [
    "print(len(embed_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.2",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
